# Define pipeline stages
stages:
  - setup
  - test
  - build
  - deploy
  - docs
  - cleanup

# Global variables
variables:
  # Cache configuration
  PIP_CACHE_DIR: "${CI_PROJECT_DIR}/.cache/pip"
  VENV_PATH: "${CI_PROJECT_DIR}/venv"
  COMPOSIO_CACHE_DIR: "${CI_PROJECT_DIR}/.composio"
  NODE_VERSION: "21"
  PYTHON_VERSION: "3.10"
  
  # Disable pip version check for faster execution
  PIP_DISABLE_PIP_VERSION_CHECK: "1"
  PIP_NO_PYTHON_VERSION_WARNING: "1"
  PYTHONUNBUFFERED: "1"
  
  # Default Python versions to test against
  PYTHON_VERSIONS: "3.10"
  
  # Default test configuration
  TEST_DIR: "tests"
  
  # Docker image for Python jobs
  PYTHON_IMAGE: "python:${PYTHON_VERSION}-slim"
  
  # Docker image for Node.js jobs
  NODE_IMAGE: "node:${NODE_VERSION}-slim"

# Cache configuration
cache:
  # Main cache for Python dependencies
  - key: "${CI_COMMIT_REF_SLUG}-python"
    paths:
      - ${PIP_CACHE_DIR}
      - .mypy_cache/
      - .pytest_cache/
    policy: pull-push
    when: on_success
    
  # Cache for Node.js dependencies
  - key: "${CI_COMMIT_REF_SLUG}-node"
    paths:
      - node_modules/
      - .npm/
    policy: pull-push
    when: on_success
    
  # Cache for Python virtual environment (only for setup-python job)
  - key: "${CI_COMMIT_REF_SLUG}-venv"
    paths:
      - ${VENV_PATH}/
    policy: pull-push
    when: on_success

# Optimize Docker image pulling
image:
  name: ${PYTHON_IMAGE}
  pull_policy: if-not-present

# Default before_script for Python jobs
.before_script: &before_script
  - python --version
  - pip install --upgrade pip setuptools wheel
  - python -m venv ${VENV_PATH}
  - source ${VENV_PATH}/bin/activate
  # Install aiexec-base from private repository
  - pip install --extra-index-url ${PYPI_EXTRA_INDEX_URL} aiexec-base~=1.0.0
  # Install the rest of the package in development mode
  - pip install -e .
  - mkdir -p ${COMPOSIO_CACHE_DIR}

# Before script for Node.js jobs
.before_script_node: &before_script_node
  - node --version
  - npm --version
  - npm ci

# Job to check if we should run the pipeline
check-nightly-status:
  stage: setup
  image: python:3.10-slim
  script:
    - apt-get update && apt-get install -y curl jq
    - |
      # Get today's date in ISO format for comparison
      TODAY=$(date -u +"%Y-%m-%d")
      echo "Today's date: $TODAY"

      # Query PyPI API for the aiexec package
      HTTP_STATUS=$(curl -s -o response.json -w "%{http_code}" https://pypi.org/pypi/aiexec-nightly/json)

      # Check HTTP status code first
      if [ "$HTTP_STATUS" -ne 200 ]; then
        echo "Error: PyPI API returned HTTP status $HTTP_STATUS"
        exit 0
      fi

      # Check if response is valid JSON before proceeding
      if ! jq -e . response.json >/dev/null 2>&1; then
        echo "Error: Invalid JSON response from PyPI API"
        echo "Response preview:"
        head -n 10 response.json
        exit 0
      fi

      # Extract the latest version
      LATEST_VERSION=$(jq -r '.info.version // empty' response.json)

      if [ -z "$LATEST_VERSION" ]; then
        echo "Could not extract latest version"
        exit 0
      fi

      # Extract the release date of the latest version
      RELEASE_DATE=$(jq -r --arg ver "$LATEST_VERSION" '.releases[$ver][0].upload_time_iso_8601 // empty' response.json | cut -d'T' -f1)

      if [ -z "$RELEASE_DATE" ]; then
        echo "Could not extract release date"
        exit 0
      fi

      echo "Latest version: $LATEST_VERSION"
      echo "Release date: $RELEASE_DATE"

      # Check if the release date is today
      if [[ "$RELEASE_DATE" == "$TODAY" ]]; then
        echo "Package was updated today"
        exit 0
      else
        echo "Package was not updated today"
        exit 1
      fi
  artifacts:
    when: always
    paths:
      - response.json
    expire_in: 1 week
  only:
    - schedules  # Only run on schedule, not on every push

# Job to determine which parts of the codebase have changed
path-filter:
  stage: setup
  image: python:3.10-slim
  script:
    - |
      # Initialize variables
      echo "PYTHON_CHANGED=false" > build_vars.env
      echo "FRONTEND_CHANGED=false" >> build_vars.env
      echo "DOCS_CHANGED=false" >> build_vars.env
      
      # Get changed files
      if [ "$CI_PIPELINE_SOURCE" == "merge_request_event" ]; then
        # For merge requests, compare with the target branch
        CHANGED_FILES=$(git diff --name-only $CI_MERGE_REQUEST_TARGET_BRANCH_SHA...$CI_COMMIT_SHA)
      else
        # For branches, compare with the previous commit
        CHANGED_FILES=$(git diff --name-only HEAD~1 HEAD)
      fi
      
      # Check for Python changes
      if echo "$CHANGED_FILES" | grep -qE '(\.py$|pyproject\.toml|requirements.*\.txt|setup\.py)'; then
        echo "PYTHON_CHANGED=true" >> build_vars.env
      fi
      
      # Check for frontend changes
      if echo "$CHANGED_FILES" | grep -qE '(\.(js|jsx|ts|tsx|css|scss|html)$|package\.json|yarn\.lock|vite\.config\.(js|ts))'; then
        echo "FRONTEND_CHANGED=true" >> build_vars.env
      fi
      
      # Check for documentation changes
      if echo "$CHANGED_FILES" | grep -qE '(^docs/|\.(md|mdx)$)'; then
        echo "DOCS_CHANGED=true" >> build_vars.env
      fi
      
      # Output the results
      echo "=== Changed Files ==="
      echo "$CHANGED_FILES"
      echo "\n=== Build Variables ==="
      cat build_vars.env
  artifacts:
    reports:
      dotenv: build_vars.env
    paths:
      - build_vars.env
    expire_in: 1 hour

# Job to set up the Python environment
setup-python:
  stage: setup
  image: ${PYTHON_IMAGE}
  script:
    - *before_script
    - pip install -r requirements-dev.txt
  artifacts:
    paths:
      - ${VENV_PATH}/
    expire_in: 1 week
  cache:
    key: "${CI_COMMIT_REF_SLUG}-python"
    paths:
      - ${VENV_PATH}/
    policy: pull-push

# Backend test job
test-backend:
  stage: test
  image: ${PYTHON_IMAGE}
  needs: 
    - job: setup-python
      artifacts: true
    - job: path-filter
      artifacts: true
  variables:
    GIT_DEPTH: 0
    PYTHONUNBUFFERED: 1
    PYTHONDONTWRITEBYTECODE: 1
    # Enable parallel test execution
    PYTEST_ADDOPTS: "-n auto"
  cache:
    key: "${CI_COMMIT_REF_SLUG}-python"
    paths:
      - .pytest_cache/
      - .coverage
    policy: pull-push
  before_script:
    - *before_script
    # Only install test dependencies if they're not already installed
    - pip install -r requirements-dev.txt
  script:
    - echo "Running backend tests with $(nproc) workers..."
    - pytest src/backend/tests/ 
      -v 
      --cov=src/backend 
      --cov-report=xml
      --cov-report=term
      --durations=10
  artifacts:
    when: always
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml
    paths:
      - coverage.xml
      - .coverage
    expire_in: 1 week
  retry:
    max: 2
    when:
      - script_failure
  rules:
    - if: $PYTHON_CHANGED == "true"
      when: on_success
    - when: never

# Frontend unit tests
frontend-unit-tests:
  stage: test
  image: ${NODE_IMAGE}
  needs: 
    - job: path-filter
      artifacts: true
  cache:
    key: "${CI_COMMIT_REF_SLUG}-node"
    paths:
      - node_modules/
      - .cache/
    policy: pull-push
  variables:
    NODE_ENV: test
    CI: "true"
    # Enable parallel test execution
    NODE_OPTIONS: "--max_old_space_size=4096"
  before_script:
    - *before_script_node
    # Install only production dependencies first for faster installs
    - npm ci --prefer-offline --cache .npm --no-audit
  script:
    - echo "Running frontend unit tests with $(nproc) workers..."
    - npm run test:unit -- --maxWorkers=2 --coverage
  artifacts:
    when: always
    paths:
      - coverage/
      - test-results/
    reports:
      junit: test-results/junit.xml
      coverage_report:
        coverage_format: cobertura
        path: coverage/cobertura-coverage.xml
    expire_in: 1 week
  retry:
    max: 1
    when:
      - script_failure
  rules:
    - if: $FRONTEND_CHANGED == "true"
      when: on_success
    - when: never

# Frontend integration tests
frontend-tests:
  stage: test
  image: ${NODE_IMAGE}
  needs: [path-filter]
  before_script:
    - *before_script_node
  script:
    - echo "Running frontend integration tests..."
    - npm run test:integration
  artifacts:
    when: always
    paths:
      - cypress/screenshots/
      - cypress/videos/
    reports:
      junit: cypress/results/*.xml
  rules:
    - if: $FRONTEND_CHANGED == "true"
      when: on_success
    - when: never

# Documentation build test
test-docs-build:
  stage: docs
  image: ${NODE_IMAGE}
  needs: [path-filter]
  before_script:
    - *before_script_node
  script:
    - echo "Testing documentation build..."
    - npm ci
    - npm run docs:build
  artifacts:
    paths:
      - docs/build/
    expire_in: 1 week
  rules:
    - if: $DOCS_CHANGED == "true"
      when: on_success
    - when: never

# Linting job for Python
lint-python:
  stage: test
  image: ${PYTHON_IMAGE}
  needs: [setup-python, path-filter]
  script:
    - *before_script
    - echo "Running Python linters..."
    - black --check --diff .
    - flake8 .
    - mypy .
  rules:
    - if: $PYTHON_CHANGED == "true"
      when: on_success
    - when: never

# Linting job for JavaScript/TypeScript
lint-frontend:
  stage: test
  image: ${NODE_IMAGE}
  needs: [path-filter]
  before_script:
    - *before_script_node
  script:
    - echo "Running frontend linters..."
    - npm run lint
  rules:
    - if: $FRONTEND_CHANGED == "true"
      when: on_success
    - when: never

# Test starter templates
test-templates:
  stage: test
  image: ${PYTHON_IMAGE}
  needs: [setup-python, path-filter]
  script:
    - *before_script
    - pip install uv
    - uv pip install -r requirements-dev.txt
    - echo "Testing starter project templates..."
    - pytest src/backend/tests/unit/template/test_starter_projects.py -v -n auto
  rules:
    - if: $PYTHON_CHANGED == "true" || $FRONTEND_CHANGED == "true"
      when: on_success
    - when: never

# Final success/failure notification
ci-success:
  stage: cleanup
  image: alpine:latest
  needs:
    - job: test-backend
      optional: true
    - job: frontend-unit-tests
      optional: true
    - job: frontend-tests
      optional: true
    - job: lint-python
      optional: true
    - job: lint-frontend
      optional: true
    - job: test-templates
      optional: true
    - job: test-docs-build
      optional: true
  script:
    - |
      echo "=== CI Status Summary ==="
      echo "Backend tests: $CI_JOB_STATUS"
      echo "Frontend unit tests: $CI_JOB_STATUS"
      echo "Frontend integration tests: $CI_JOB_STATUS"
      echo "Python linting: $CI_JOB_STATUS"
      echo "Frontend linting: $CI_JOB_STATUS"
      echo "Template tests: $CI_JOB_STATUS"
      echo "Documentation build: $CI_JOB_STATUS"
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
    - if: $CI_COMMIT_BRANCH == "master"
    - black --check --diff .
    - echo "Running flake8..."
    - flake8 .
    - echo "Running mypy..."
    - mypy .
  artifacts:
    when: on_failure
    paths:
      - ${COMPOSIO_CACHE_DIR}/
    expire_in: 1 week
  cache:
    key: "${CI_COMMIT_REF_SLUG}-lint"
    paths:
      - ${PIP_CACHE_DIR}
      - ${VENV_PATH}
      - .mypy_cache/
    policy: pull-push

# Testing job
test:
  stage: test
  before_script:
    - *before_script
    - pip install pytest==7.4.0 pytest-cov==4.1.0
  script:
    - echo "Running tests with coverage..."
    - pytest --cov=./ --cov-report=xml --cov-report=term-missing
  coverage: '/^TOTAL.*\s+(\d+\%)$/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml
    paths:
      - coverage.xml
      - .coverage
    expire_in: 1 week
  cache:
    key: "${CI_COMMIT_REF_SLUG}-test"
    paths:
      - ${PIP_CACHE_DIR}
      - ${VENV_PATH}
      - .pytest_cache/
    policy: pull-push

# Build job
build:
  stage: build
  before_script:
    - *before_script
    - pip install build==0.10.0
  script:
    - echo "Building Python package..."
    - python -m build --sdist --wheel --outdir dist/ .
  artifacts:
    paths:
      - dist/*
    expire_in: 1 week
  only:
    - main
    - merge_requests
    - tags

# Deploy to staging
deploy_staging:
  stage: deploy
  before_script:
    - *before_script
  script:
    - echo "Deploying to staging environment..."
    # Example: twine upload --repository-url ${STAGING_PYPI_URL} -u ${STAGING_PYPI_USER} -p ${STAGING_PYPI_PASSWORD} dist/*
    - echo "Deployment to staging completed successfully"
  environment:
    name: staging
    url: https://staging.example.com  # Update with your staging URL
  only:
    - main  # Only deploy when changes are pushed to main branch
  cache: {}
  dependencies:
    - build

# Deploy to production
deploy_prod:
  stage: deploy
  before_script:
    - *before_script
  script:
    - echo "Deploying to production..."
    # Example: twine upload -u ${PYPI_USER} -p ${PYPI_PASSWORD} dist/*
    - echo "Production deployment completed successfully"
  environment:
    name: production
    url: https://example.com  # Update with your production URL
  when: manual  # Requires manual approval in GitLab UI
  only:
    - tags  # Only deploy when a new tag is created
  cache: {}
  dependencies:
    - build

# Workflow rules to control pipeline behavior
workflow:
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
    - if: $CI_COMMIT_BRANCH =~ /^feature\/.*/ && $CI_PIPELINE_SOURCE == 'merge_request_event'
    - if: $CI_COMMIT_TAG
    - if: $CI_COMMIT_BRANCH =~ /^release\/.*/
